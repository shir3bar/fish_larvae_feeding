{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import FPS\n",
    "from imutils.video import FileVideoStream\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "from imutils.video import count_frames \n",
    "SHAPE = [1080,1920]\n",
    "vid_file='03.06.20 13DPH.mp4'#'11.12.2019 - frames 64935-65415.avi'#\n",
    "output_vid_name='trial1.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(x, y, w, h):\n",
    "    \"\"\" Get bounding box centroid\"\"\"\n",
    "    x1 = int(w / 2)\n",
    "    y1 = int(h / 2)\n",
    "\n",
    "    cx = x + x1\n",
    "    cy = y + y1\n",
    "\n",
    "    return (cx, cy)\n",
    "\n",
    "def get_contours(image,min_width=50,min_height=50):\n",
    "    \"\"\" Get the blobs or contours detected in the image\n",
    "    input: \n",
    "    image - a frame\n",
    "    min_width - the minimal width of a blob, all blobs below threshold will be disregarded\n",
    "    min_height - the minimal width of a blob, all blobs below threshold will be disregarded\n",
    "    returns: \n",
    "    matches - a list of tuples, each tuple contains two tuples with information about each detected blob.\n",
    "        first tuple is the bounding box coordinates, width and height \n",
    "        second tuple contains the centroid coordinates\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    # find all contours/blobs in the frame:\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1)\n",
    "    for (i, contour) in enumerate(contours):\n",
    "        # for each detected object/blob/contour\n",
    "        # Get the bounding box:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        # Filter by height and width:\n",
    "        contour_valid = (w >= min_width) and (\n",
    "                    h >= min_height)\n",
    "\n",
    "        if not contour_valid:\n",
    "            continue\n",
    "        # Get bbox centroid:\n",
    "        centroid = get_centroid(x, y, w, h)\n",
    "        matches.append(((x, y, w, h), centroid))\n",
    "    return matches\n",
    "\n",
    "def draw_boxes(img, pathes):\n",
    "    \"\"\" Draw bounding boxes around objects in image\n",
    "    \"\"\"\n",
    "    BOUNDING_BOX_COLOUR = (255, 10, 0) #BGR\n",
    "    CENTROID_COLOUR = (255, 192, 0) #BGR\n",
    "    for (i, match) in enumerate(pathes):\n",
    "        contour, centroid = match\n",
    "        x, y, w, h = contour\n",
    "\n",
    "        cv2.rectangle(img, (x, y), (x + w - 1, y + h - 1),\n",
    "                          BOUNDING_BOX_COLOUR, 4)\n",
    "        cv2.circle(img, centroid, 2, CENTROID_COLOUR, -1)\n",
    "    return img\n",
    "\n",
    "def get_filter(frame,bg_sub,brighten=True,blur=False):\n",
    "    \"\"\" Get the foreground mask for a frame\n",
    "    input:\n",
    "    frame - frame from video\n",
    "    bg_sub - an open-cv pre-trained background subtractor\n",
    "    brighten - bool, whether to adjust brightness in the image\n",
    "    blur - bool, whether to apply gaussian blur to remove background noise from the image\n",
    "    output:\n",
    "    combined - a foreground mask created by fine-tuning the background subtractor output with edge detection.\n",
    "    \"\"\"\n",
    "    gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Turn to grayscale \n",
    "    denoise=cv2.GaussianBlur(gray,(71,71),0) \n",
    "    if blur:\n",
    "        # Apply gaussian blur to image\n",
    "        gray=denoise\n",
    "    if brighten:\n",
    "        # Apply brightness adjustment to image\n",
    "        gray=cv2.convertScaleAbs(gray, alpha=1, beta=50)\n",
    "    fg_mask = bg_sub.apply(gray, None, 0.001) # calculate foreground mask\n",
    "    img=cv2.Canny(denoise,10,10) # get edges from the blurred image to filter out small floating particles\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10,10)) \n",
    "    closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel) # fill in gaps in the edges\n",
    "    # get the areas that are detected by both the bg-sub and the edge detection routine:\n",
    "    combined=cv2.bitwise_and(fg_mask,closing) \n",
    "    return combined\n",
    "\n",
    "def train_bg_subtractor(cap, bg_sub, num, brighten=True, blur=False):\n",
    "    \"\"\" Pre-train the background subtractor.\n",
    "    input:\n",
    "    cap - video capture object, the video\n",
    "    bg_sub - background subtractor object\n",
    "    num - number of frames to train on\n",
    "    brighten - bool, whether to adjust brightness in the image\n",
    "    blur - bool, whether to apply gaussian blur to remove background noise from the image\n",
    "    output:\n",
    "    bg_sub - trained background subtractor\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        # iterate over the selected number of frames\n",
    "        #ret,frame=cap.read() # get one frame\n",
    "        _,frame=cap.read()\n",
    "        gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert to grayscale \n",
    "        if brighten:\n",
    "            # modify brightness if applicable:\n",
    "            gray=cv2.convertScaleAbs(gray, alpha=1, beta=50)\n",
    "        if blur:\n",
    "            # apply gaussian blur if applicable:\n",
    "            gray=cv2.GaussianBlur(gray,(71,71),0)\n",
    "        fg_mask=bg_sub.apply(gray, None, 0.001) #apply bg_sub to the frame (modified or not)\n",
    "    return bg_sub \n",
    "\n",
    "def save_frame(frame, file_name, flip=True):\n",
    "    \"\"\" Save  frame as image\"\"\"\n",
    "    # flip BGR to RGB\n",
    "    if flip:\n",
    "        cv2.imwrite(file_name, np.flip(frame, 2))\n",
    "    else:\n",
    "        cv2.imwrite(file_name, frame)\n",
    "        \n",
    "        \n",
    "def process_vid(vid_name,output_vid,brighten=True,blur=False):\n",
    "    \"\"\" Detect objects in a single video and create a new video with the bounding boxes around objects.\n",
    "    input:\n",
    "    vid_name - path and filename of the input video\n",
    "    output_vid - path and filename for the generated video\n",
    "    brighten - bool, whether to adjust brightness in the image\n",
    "    blur - bool, whether to apply gaussian blur to remove background noise from the image\n",
    "    \"\"\"\n",
    "    # count the number of frames in the video, so we can determine the portion dedicated for training\n",
    "    # the background subtractor:\n",
    "    num_frames=count_frames(vid_name) \n",
    "    cap=cv2.VideoCapture(vid_name) # open video\n",
    "    train_frames=round(num_frames/4) # number of frames to pre-train the bg_sub on\n",
    "    bg_sub=cv2.createBackgroundSubtractorMOG2(history=train_frames,detectShadows=True) \n",
    "    bg_sub=train_bg_subtractor(cap, bg_sub,train_frames) # pre-training\n",
    "    images=[]\n",
    "    # codec for video writing, see: https://www.pyimagesearch.com/2016/02/22/writing-to-video-with-opencv/\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") \n",
    "    # the video writer is defined by output filename, selected codec, the fps selected is the same as the input video, \n",
    "    # frame size that is also identical to input video, and a boolean specific whether to save a color video:\n",
    "    writer = cv2.VideoWriter(output_vid, fourcc, cap.get(cv2.CAP_PROP_FPS), (SHAPE[1], SHAPE[0]), True)\n",
    "    # iterate over the remaining frames:\n",
    "    while True:\n",
    "        grabbed,frame=cap.read() # get frame\n",
    "        if not grabbed:\n",
    "            break\n",
    "        mask=get_filter(frame,bg_sub) # get foreground mask\n",
    "        matches=get_contours(mask) # find objects inside the mask, get a list of their bounding boxes\n",
    "        img1=[] \n",
    "        img1=draw_boxes(frame,matches) # draw bounding boxes on original frame\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) # optional: open-cv work in BGR so convert back to RGB\n",
    "        writer.write(img1) # Write frame to the new video file\n",
    "        #fps.update()\n",
    "   # release resources:     \n",
    "    writer.release()\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = FPS().start()\n",
    "process_vid(vid_file,output_vid_name,brighten=False,blur=True)\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut out an example vid for Roi\n",
    "c=cv2.VideoCapture(output_vid_name)\n",
    "images=[]\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") \n",
    "writer = cv2.VideoWriter('fish_example.avi', fourcc, c.get(cv2.CAP_PROP_FPS), (SHAPE[1], SHAPE[0]), True)\n",
    "\n",
    "for i in range(30):\n",
    "    _,frame=c.read()\n",
    "    #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    writer.write(frame)\n",
    "    #images.append(Image.fromarray(frame))\n",
    "writer.release()\n",
    "c.release()\n",
    "#images[0].save('fish.gif',\n",
    " #              save_all=True, append_images=images[1:], optimize=False, duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video file thread...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.3.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-225258e4d2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbg_sub\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get foreground mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmatches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_contours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# find objects inside the mask, get a list of their bounding boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mimg1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-7c97cd6fb6ef>\u001b[0m in \u001b[0;36mget_filter\u001b[0;34m(frame, bg_sub, brighten, blur)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mcombined\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m \u001b[0mforeground\u001b[0m \u001b[0mmask\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtuning\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbackground\u001b[0m \u001b[0msubtractor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0medge\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mgray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Turn to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mdenoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m71\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m71\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.3.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"[INFO] starting video file thread...\")\n",
    "num_frames=count_frames(vid_file) \n",
    "train_frames=round(num_frames/4) # number of frames to pre-train the bg_sub on\n",
    "bg_sub=cv2.createBackgroundSubtractorMOG2(history=train_frames,detectShadows=True) \n",
    "bg_sub=train_bg_subtractor(fvs, bg_sub,train_frames)\n",
    "fvs = FileVideoStream(vid_file).start()\n",
    "time.sleep(1.0)\n",
    "# start the FPS timer\n",
    "\n",
    "fps = FPS().start()# loop over frames from the video file stream\n",
    "while fvs.more():\n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale (while still retaining 3\n",
    "    # channels)\n",
    "    frame = fvs.read()\n",
    "    mask=get_filter(frame,bg_sub) # get foreground mask\n",
    "    matches=get_contours(mask) # find objects inside the mask, get a list of their bounding boxes\n",
    "    img1=[] \n",
    "    img1=draw_boxes(frame,matches) # draw bounding boxes on original frame\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) # optional: open-cv work in BGR so convert back to RGB\n",
    "     #   writer.write(img1) # Write frame to the new video file\n",
    "    #fps.update()\n",
    "    # display the size of the queue on the frame\n",
    "    #cv2.putText(frame, \"Queue Size: {}\".format(fvs.Q.qsize()),\n",
    "     #   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    # show the frame and update the FPS counter\n",
    "    #cv2.imshow(\"Frame\", frame)\n",
    "    #cv2.waitKey(1)\n",
    "    fps.update()\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "fvs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() missing 1 required positional argument: 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8aa2d1e1010b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: new() missing 1 required positional argument: 'size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_frames('fish_example.avi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-07a535307e97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'frame' is not defined"
     ]
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
